{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Is the NBA Becoming a More Global Product: An Analysis of NBA Player Distribution and Trends**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Introduction**\n",
    "\n",
    "This project explores the globalisation of the NBA by analysing advanced statistics, player distribution, and trends over time. Using advanced metrics, each player is assigned a composite score to quantify their performance for a given season. By filtering for the top 10 NBA players over the past three seasons based on this composite score, this study aims to evaluate whether globalisation is permeating the league’s elite ranks. Furthermore, this project examines player distribution by nationality over the same period to assess the broader impact of globalisation on the league.\n",
    "\n",
    "**Key Questions:**\n",
    "1. Who are the top 10 players in the NBA based on performance over the past three seasons?\n",
    "2. How has the distribution of NBA players by nationality evolved over the same period?\n",
    "\n",
    "This analysis leverages data from two primary sources:\n",
    "- **[Basketball-Reference.com](https://www.basketball-reference.com):** A widely respected resource for historical and current NBA data, providing comprehensive player statistics and advanced metrics.\n",
    "- **[NBA API](https://developer.nba.com/):** An official source of real-time league data, offering up-to-date player metrics and team statistics for robust and accurate analysis.\n",
    "\n",
    "By examining these questions and trends, this study seeks to determine whether the influx of international players is reshaping the league and assess whether these players are competing at the highest levels of basketball excellence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Import modules**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import requests\n",
    "import os \n",
    "import sys\n",
    "from bs4 import BeautifulSoup\n",
    "from io import StringIO, BytesIO\n",
    "from unidecode import unidecode\n",
    "import unicodedata\n",
    "from nba_api.stats.endpoints import leaguedashplayerstats, commonplayerinfo\n",
    "from nba_api.stats.library.parameters import SeasonTypeAllStar\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import psycopg2\n",
    "from psycopg2 import OperationalError\n",
    "from sqlalchemy import create_engine\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Read Environment Variables from Config File**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables manually\n",
    "env_file = \"config.env\"\n",
    "if os.path.exists(env_file):\n",
    "    with open(env_file) as f:\n",
    "        for line in f:\n",
    "            key, value = line.strip().split(\"=\")\n",
    "            os.environ[key] = value  # Store in environment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. Data Collection**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basketball-Reference Data Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transliterate_name(name: str) -> str:\n",
    "    \"\"\"\n",
    "    Ensures names are alphabetic and free of special characters or accents.\n",
    "    \"\"\"\n",
    "    if isinstance(name, str):\n",
    "        name = unicodedata.normalize(\"NFKD\", name)\n",
    "        name = unidecode(name)\n",
    "        name = ''.join(c for c in name if c.isalpha() or c.isspace()).strip()\n",
    "    return name\n",
    "\n",
    "\n",
    "def ensure_folder_exists(folder_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Ensures the specified folder exists.\n",
    "    \"\"\"\n",
    "    os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "\n",
    "def scrape_save_stats(season: str, folder_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Scrapes advanced stats from Basketball-Reference for a given season and saves HTML files.\n",
    "    Returns the file path of the saved HTML.\n",
    "    \"\"\"\n",
    "    ensure_folder_exists(folder_path)\n",
    "    file_name = f\"bball_ref_20{season.split('-')[1]}.html\"\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"Scraping data for the {season} season...\")\n",
    "        url = f\"https://www.basketball-reference.com/leagues/NBA_20{season.split('-')[1]}_advanced.html\"\n",
    "        response = requests.get(url)\n",
    "        response.encoding = 'utf-8'\n",
    "        response.raise_for_status()\n",
    "\n",
    "        with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(response.text)\n",
    "        print(f\"File saved: {file_path}\")\n",
    "    else:\n",
    "        print(f\"Data already saved for {season} season.\")\n",
    "\n",
    "    return file_path\n",
    "\n",
    "\n",
    "def parse_and_clean(file_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Parses an HTML file, extracts and cleans the advanced stats table, and returns a DataFrame.\n",
    "    \"\"\"\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        page = f.read()\n",
    "\n",
    "    soup = BeautifulSoup(page, \"html5lib\")\n",
    "    table = soup.find(\"table\", {\"id\": \"advanced\"})\n",
    "    if not table:\n",
    "        raise ValueError(f\"No table found in file: {file_path}\")\n",
    "\n",
    "    df = pd.read_html(StringIO(str(table)))[0]\n",
    "    df = df[df[\"Player\"] != \"Player\"]  # Remove duplicate headers\n",
    "    df = df[[\"Player\", \"Team\", \"G\", \"PER\", \"TS%\", \"USG%\", \"WS/48\", \"BPM\", \"VORP\"]]\n",
    "    df.columns = [\"player\", \"team\", \"games_played\", \"per\", \"ts\", \"usg\", \"ws_48\", \"bpm\", \"vorp\"]\n",
    "    df[\"player\"] = df[\"player\"].str.strip().apply(transliterate_name)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def collect_scraped_data(seasons: list[str], folder_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Collects and compiles scraped advanced stats data for the specified seasons.\n",
    "    \"\"\"\n",
    "    all_stats = []\n",
    "    for season in seasons:\n",
    "        try:\n",
    "            file_path = scrape_save_stats(season, folder_path)\n",
    "            season_data = parse_and_clean(file_path)\n",
    "            season_data[\"season\"] = season\n",
    "            all_stats.append(season_data)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing data for {season}: {e}\")\n",
    "\n",
    "    if not all_stats:\n",
    "        raise ValueError(\"No valid data collected for any season.\")\n",
    "\n",
    "    return pd.concat(all_stats, ignore_index=True)\n",
    "\n",
    "\n",
    "last_3_seasons = [\"2023-24\", \"2022-23\", \"2021-22\"]\n",
    "folder_path = \"./data/bball_ref\"\n",
    "\n",
    "bball_ref_df = collect_scraped_data(last_3_seasons, folder_path)\n",
    "\n",
    "print(\"Data collection and cleaning completed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `bball_ref_df` includes the following columns:  \n",
    "\n",
    "| **Column**                       | **Description**                                                                 |\n",
    "|----------------------------------|---------------------------------------------------------------------------------|\n",
    "| Player                        | Full name of player                                                                      |\n",
    "| Team                        | 3-letter team label (e.g. LAL)                                                 |\n",
    "| Season                      | NBA season (e.g. \"2023-24\")                                                    |\n",
    "| Games Played                | Number of games played in the season                                           |\n",
    "| Player Efficiency Rating (PER) | A per-minute rating summarising accomplishments, adjusted for pace; avg = 15   |\n",
    "| True Shooting Percentage (TS%) | Accounts for field goals, three-point shots, and free throws                   |\n",
    "| Usage Percentage (USG%)     | Percentage of team plays used, including FG attempts, FT attempts, and turnovers |\n",
    "| Win Shares per 48 Minutes (WS/48)| Contribution to team wins, normalised to 48 minutes; league avg = 0.100       |\n",
    "| Box Plus/Minus (BPM)        | Impact on team's performance per 100 possessions compared to a league-average player |\n",
    "| Value Over Replacement (VORP) | Contribution above replacement-level player, scaled per season                 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **NBA API Data Collection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_folder_exists(folder_path: str) -> None:\n",
    "    \"\"\"Ensures the specified folder exists.\"\"\"\n",
    "    try:\n",
    "        os.makedirs(folder_path, exist_ok=True)\n",
    "        print(f\"Folder checked/created: {folder_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error ensuring folder exists: {e}\")\n",
    "        raise\n",
    "\n",
    "def load_nationality_cache(nationality_filepath: str) -> dict:\n",
    "    \"\"\"Loads the nationality cache from a CSV file if it exists.\"\"\"\n",
    "    if os.path.exists(nationality_filepath):\n",
    "        try:\n",
    "            nationality_df = pd.read_csv(nationality_filepath)\n",
    "            print(f\"Loaded player nationalities from cache: {len(nationality_df)} players.\")\n",
    "            return nationality_df.set_index(\"player_id\")[\"nationality\"].to_dict()\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading nationality cache: {e}\")\n",
    "            return {}\n",
    "    print(\"No nationality cache found; starting fresh.\")\n",
    "    return {}\n",
    "\n",
    "def save_nationality_cache(nationality_filepath: str, new_nationalities: list[dict]) -> None:\n",
    "    \"\"\"Saves the updated nationality cache to a CSV file.\"\"\"\n",
    "    if new_nationalities:\n",
    "        try:\n",
    "            new_nationalities_df = pd.DataFrame(new_nationalities)\n",
    "            if os.path.exists(nationality_filepath):\n",
    "                cached_nationalities = pd.read_csv(nationality_filepath)\n",
    "                new_nationalities_df = pd.concat(\n",
    "                    [cached_nationalities, new_nationalities_df], ignore_index=True\n",
    "                ).drop_duplicates()\n",
    "            new_nationalities_df.to_csv(nationality_filepath, index=False)\n",
    "            print(f\"Updated player nationalities saved to {nationality_filepath}.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving nationality cache: {e}\")\n",
    "            raise\n",
    "\n",
    "def fetch_season_stats(season: str, season_filepath: str) -> pd.DataFrame:\n",
    "    \"\"\"Fetches and saves advanced stats for a given season.\"\"\"\n",
    "    if os.path.exists(season_filepath):\n",
    "        print(f\"Loading data for season {season} from cache.\")\n",
    "        return pd.read_csv(season_filepath)\n",
    "\n",
    "    print(f\"Fetching data for season {season}...\")\n",
    "    try:\n",
    "        stats = leaguedashplayerstats.LeagueDashPlayerStats(\n",
    "            season=season,\n",
    "            season_type_all_star=SeasonTypeAllStar.regular,\n",
    "            measure_type_detailed_defense=\"Advanced\",\n",
    "            per_mode_detailed=\"PerGame\"\n",
    "        ).get_data_frames()[0]\n",
    "\n",
    "        # Keep only relevant columns\n",
    "        stats = stats[[\"PLAYER_ID\", \"PLAYER_NAME\", \"TEAM_ABBREVIATION\", \"GP\", \"MIN\", \"OFF_RATING\", \"DEF_RATING\"]]\n",
    "        stats.columns = [\"player_id\", \"player\", \"team\", \"games_played\", \"min\", \"off_rating\", \"def_rating\"]\n",
    "        stats[\"season\"] = season\n",
    "\n",
    "        # Save to CSV\n",
    "        stats.to_csv(season_filepath, index=False)\n",
    "        print(f\"Season {season} data saved to {season_filepath}.\")\n",
    "        return stats\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching data for season {season}: {e}\")\n",
    "        return pd.DataFrame()  # Return an empty DataFrame on failure\n",
    "\n",
    "def fetch_player_nationalities(unique_player_ids: set, player_nationality_cache: dict) -> list[dict]:\n",
    "    \"\"\"Fetches nationalities for unique player IDs and updates the cache.\"\"\"\n",
    "    new_nationalities = []\n",
    "\n",
    "    for player_id in unique_player_ids:\n",
    "        if player_id not in player_nationality_cache:\n",
    "            try:\n",
    "                player_info = commonplayerinfo.CommonPlayerInfo(player_id=player_id).get_data_frames()[0]\n",
    "                nationality = player_info.loc[0, \"COUNTRY\"]\n",
    "                player_nationality_cache[player_id] = nationality\n",
    "                new_nationalities.append({\"player_id\": player_id, \"nationality\": nationality})\n",
    "                print(f\"Fetched nationality for player ID {player_id}: {nationality}\")\n",
    "                time.sleep(1)  # Avoid hitting rate limits\n",
    "            except Exception as e:\n",
    "                print(f\"Error fetching nationality for player ID {player_id}: {e}\")\n",
    "                player_nationality_cache[player_id] = \"unknown\"\n",
    "                new_nationalities.append({\"player_id\": player_id, \"nationality\": \"unknown\"})\n",
    "\n",
    "    return new_nationalities\n",
    "\n",
    "def fetch_advanced_stats_with_nationality(seasons: list[str], folder_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Fetches advanced stats and player nationalities, saves data to avoid redundant API calls.\n",
    "    \"\"\"\n",
    "    ensure_folder_exists(folder_path)\n",
    "    nationality_filepath = os.path.join(folder_path, \"player_nationality.csv\")\n",
    "    player_nationality_cache = load_nationality_cache(nationality_filepath)\n",
    "\n",
    "    all_stats = []\n",
    "    unique_player_ids = set()\n",
    "\n",
    "    for season in seasons:\n",
    "        season_filepath = os.path.join(folder_path, f\"nba_stats_{season.replace('-', '_')}.csv\")\n",
    "        season_stats = fetch_season_stats(season, season_filepath)\n",
    "        if not season_stats.empty:\n",
    "            all_stats.append(season_stats)\n",
    "            unique_player_ids.update(season_stats[\"player_id\"].unique())\n",
    "        time.sleep(2)  # Avoid hitting rate limits\n",
    "\n",
    "    new_nationalities = fetch_player_nationalities(unique_player_ids, player_nationality_cache)\n",
    "    save_nationality_cache(nationality_filepath, new_nationalities)\n",
    "\n",
    "    if not all_stats:\n",
    "        print(\"No data was fetched for the specified seasons.\")\n",
    "        raise ValueError(\"No data was fetched for the specified seasons.\")\n",
    "\n",
    "    combined_stats = pd.concat(all_stats, ignore_index=True)\n",
    "    combined_stats[\"nationality\"] = combined_stats[\"player_id\"].map(player_nationality_cache)\n",
    "\n",
    "    return combined_stats\n",
    "\n",
    "# Execution logic for the notebook\n",
    "last_3_seasons = [\"2023-24\", \"2022-23\", \"2021-22\"]\n",
    "folder_path = \"./data/nba_api\"\n",
    "\n",
    "nba_api_df = fetch_advanced_stats_with_nationality(last_3_seasons, folder_path)\n",
    "nba_api_df[\"player\"] = nba_api_df[\"player\"].str.strip().apply(transliterate_name)\n",
    "\n",
    "print(\"Data fetching complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `nba_api_df` DataFrame includes the following columns:\n",
    "\n",
    "| **Column**                       | **Description**                                                                 |\n",
    "|----------------------------------|---------------------------------------------------------------------------------|\n",
    "| Player ID                        | Unique ID assigned to player                                                 |\n",
    "| Player                        | Full name of player                                                                      |\n",
    "| Team                        | 3-letter team label (e.g. LAL)                                                 |\n",
    "| Games Played                | Number of games played in the season                                           |\n",
    "| Min                | Minutes played per game                                           |\n",
    "| Offensive Rating                | Estimate of the number of points a player produces per 100 possessions                                           |\n",
    "| Defensive Rating                | Estimate of the number of points a player allows per 100 possessions                                           |\n",
    "| Season                      | NBA season (e.g. \"2023-24\")                                                    |\n",
    "| Nationality                      | Nationality of player                                                    |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Summary of Data Collected**\n",
    "\n",
    "**Sample Data for Basketball-Reference Data**\n",
    "\n",
    "| Player            | Team | Games Played | PER  | TS    | USG  | WS/48  | BPM  | VORP | Season  |\n",
    "|------------------|------|--------------|------|------|------|--------|------|------|---------|\n",
    "| DeMar DeRozan   | CHI  | 79.0         | 19.7 | 0.584 | 25.8 | 0.147  | 1.8  | 2.8  | 2023-24 |\n",
    "| Domantas Sabonis| SAC  | 82.0         | 23.2 | 0.637 | 22.2 | 0.206  | 6.5  | 6.2  | 2023-24 |\n",
    "| Coby White      | CHI  | 79.0         | 14.5 | 0.570 | 22.7 | 0.078  | -0.7 | 0.9  | 2023-24 |\n",
    "| Mikal Bridges   | BRK  | 82.0         | 14.9 | 0.560 | 24.3 | 0.070  | -0.4 | 1.2  | 2023-24 |\n",
    "| Paolo Banchero  | ORL  | 80.0         | 17.3 | 0.546 | 29.7 | 0.090  | 1.3  | 2.3  | 2023-24 |\n",
    "\n",
    "**Sample Data for NBA API Data**\n",
    "\n",
    "| player_id | Player       | Team | Games Played | Min  | Off Rating | Def Rating | Season  | Nationality |\n",
    "|-----------|-------------|------|--------------|------|------------|------------|---------|-------------|\n",
    "| 1630639   | AJ Lawson   | DAL  | 42           | 7.4  | 106.6      | 105.3      | 2023-24 | Canada      |\n",
    "| 1631260   | AJ Green    | MIL  | 56           | 11.0 | 114.0      | 110.5      | 2023-24 | USA         |\n",
    "| 1631100   | AJ Griffin  | ATL  | 20           | 8.5  | 106.0      | 120.1      | 2023-24 | USA         |\n",
    "| 203932    | Aaron Gordon| DEN  | 73           | 31.5 | 119.8      | 111.1      | 2023-24 | USA         |\n",
    "| 1628988   | Aaron Holiday| HOU  | 78           | 16.3 | 110.5      | 107.6      | 2023-24 | USA         |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. Data Cleaning**\n",
    "\n",
    "The data cleaning process involves the following steps:\n",
    "\n",
    "- **Handling Missing Values:**  \n",
    "  Records with missing values are either removed or imputed using weighted averages to ensure data completeness and reliability.\n",
    "\n",
    "- **Data Alignment:**  \n",
    "  Aligning Basketball-Reference data with NBA API data is critical for seamless downstream merging. Key differences addressed include:  \n",
    "  - **Players on Multiple Teams in One Season:**  \n",
    "    - NBA API data provides aggregated records for players who played for multiple teams in a season.  \n",
    "    - Basketball-Reference data includes both aggregated records and individual team records for such players.\n",
    "    - NBA API data records the team a player last played for during the season in the aggregated record. In contrast, Basketball-Reference uses custom labels, such as `\"3TM\"`, to indicate that a player participated for three different teams in a single season.\n",
    "  - **Team Labels:**  \n",
    "    - Team labels differ between the datasets for certain teams (e.g., Brooklyn Nets recorded as `BRK` in Basketball-Reference and `BKN` in the NBA API).  \n",
    "\n",
    "These adjustments ensure consistency across datasets and facilitate accurate analysis in subsequent steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Missing Values: Basketball-Reference Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bball_ref_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `bball_ref_df` dataframe contains 2,229 rows and 10 columns. It includes missing values in the following columns: `team`, `games_played`, `per`, `ts`, `ws_48`, `bpm`, and `vorp`. The data type of each column appears to be appropriate for analysis.\n",
    "\n",
    "To handle missing values, I plan to impute them using a weighted average based on the total number of games played. The imputation process will follow these steps:\n",
    "\n",
    "1. **Single Record Players:**  \n",
    "   Players with only one record across all seasons (i.e., those who played for a single team in only one season) will be removed from the dataset.\n",
    "\n",
    "2. **Players on Multiple Teams in a Single Season:**  \n",
    "   For players who played for multiple teams in one season, missing values will be imputed using a weighted average, calculated based on the number of games played for each team during that season.\n",
    "\n",
    "3. **Players with Mixed Records:**  \n",
    "   For players with missing data in single-team seasons who also played for multiple teams in other season(s), the missing values will be imputed using a weighted average across all their seasons. Any imputed values from step 2 (for multi-team seasons) will be incorporated into this calculation when necessary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_multi_team_players(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Finds and returns rows for players who played for multiple teams in one season.\n",
    "    \"\"\"\n",
    "    multi_team_check = df.groupby(['player', 'season'], as_index=False)['team'].nunique().query('team > 1')\n",
    "    multi_team_players = df[\n",
    "        df.set_index(['player', 'season']).index.isin(multi_team_check.set_index(['player', 'season']).index)\n",
    "    ]\n",
    "    return multi_team_players\n",
    "\n",
    "\n",
    "def impute_weighted_avg(df: pd.DataFrame, numeric_cols: list, group_by: list) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Imputes missing numeric values using a weighted average based on games played.\n",
    "    \"\"\"\n",
    "    if 'games_played' not in df.columns:\n",
    "        raise KeyError(\"'games_played' column is missing from the DataFrame.\")\n",
    "        \n",
    "    grouped = df.groupby(group_by)\n",
    "\n",
    "    for col in numeric_cols:\n",
    "        if col in df.columns and df[col].isnull().any():\n",
    "            print(f\"Imputing column: {col}\")\n",
    "            df.loc[:, col] = grouped.apply(\n",
    "                lambda group: group[col].fillna(\n",
    "                    (group['games_played'] * group[col]).sum() / group['games_played'].sum()\n",
    "                    if group['games_played'].sum() > 0 else group[col].mean()\n",
    "                ), include_groups=False\n",
    "            ).reset_index(level=group_by, drop=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def update_with_imputed_records(original_df: pd.DataFrame, imputed_df: pd.DataFrame, numeric_cols: list) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Updates the original DataFrame with imputed data for numeric columns.\n",
    "    \"\"\"\n",
    "    original_df = original_df.merge(\n",
    "        imputed_df[['player', 'season', 'team'] + numeric_cols],\n",
    "        on=['player', 'season', 'team'],\n",
    "        how='left',\n",
    "        suffixes=('', '_imputed')\n",
    "    )\n",
    "    for col in numeric_cols:\n",
    "        imputed_col = f'{col}_imputed'\n",
    "        if imputed_col in original_df.columns:\n",
    "            original_df[col] = original_df[imputed_col].where(original_df[imputed_col].notnull(), original_df[col])\n",
    "            original_df.drop(columns=[imputed_col], inplace=True)\n",
    "\n",
    "\n",
    "    return original_df\n",
    "\n",
    "\n",
    "def drop_and_impute(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Drops certain records, imputes missing values for multi-team players,\n",
    "    imputes across seasons, and adds aggregated records back.\n",
    "    \"\"\"\n",
    "    # Step 1: Remove Single-Season Players and Records with Missing Games Played\n",
    "    single_record_players = df.groupby(['player'], as_index=False)['season'].nunique().query('season == 1')\n",
    "    print(f\"Total number of players that only played one season: {single_record_players.shape[0]} players\")\n",
    "    clean_df = df[~df['player'].isin(single_record_players['player'])]\n",
    "    print(f\"Total rows after removing single-season players: {clean_df.shape[0]} rows\")\n",
    "\n",
    "    clean_df = clean_df[clean_df['games_played'].notnull()]\n",
    "    print(f\"Total rows after removing records with missing games played: {clean_df.shape[0]} rows\")\n",
    "\n",
    "    # Separate aggregate rows from non-aggregate rows\n",
    "    agg_records = clean_df[clean_df['team'].str.contains(r'^\\dTM$', na=False)]\n",
    "    non_agg_records = clean_df[~clean_df['team'].str.contains(r'^\\dTM$', na=False)]\n",
    "    print(f\"Aggregated records: {agg_records.shape[0]} rows\")\n",
    "    print(f\"Non-aggregated records: {non_agg_records.shape[0]} rows\")\n",
    "\n",
    "    # Step 2: Perform Imputations\n",
    "    numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "\n",
    "    # Impute for Multi-Team Players\n",
    "    multi_team_players = find_multi_team_players(non_agg_records)\n",
    "    print(\"Columns in multi_team_players:\", multi_team_players.columns)\n",
    "    imputed_multi_df = impute_weighted_avg(multi_team_players, numeric_cols, group_by=['player', 'season'])\n",
    "\n",
    "    # Update Non-Aggregated Records with Imputed Multi-Team Player Data\n",
    "    non_agg_records = update_with_imputed_records(non_agg_records, imputed_multi_df, numeric_cols)\n",
    "\n",
    "    # Impute Across Seasons for All Players\n",
    "    final_imputed_df = impute_weighted_avg(non_agg_records, numeric_cols, group_by=['player'])\n",
    "\n",
    "    # Update Non-Aggregated Records with Season-Level Imputations\n",
    "    non_agg_records = update_with_imputed_records(non_agg_records, final_imputed_df, numeric_cols)\n",
    "\n",
    "    # Step 3: Add Aggregated Records Back\n",
    "    clean_df = pd.concat([non_agg_records, agg_records], ignore_index=True)\n",
    "    print(\"Missing values have been dropped or imputed.\")\n",
    "\n",
    "    # Debug: Check for Remaining Missing Values\n",
    "    print(\"Final DataFrame missing values:\\n\", clean_df.isnull().sum())\n",
    "\n",
    "    return clean_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bball_imputed_df = drop_and_impute(bball_ref_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Missing Values: NBA API Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nba_api_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 1716 rows and 8 columns. There are no missing values and the datatype of each column is appropriate. Records for players who only played for one season are dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nba_dropped_df = drop_and_impute(nba_api_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Data Alignment: Team Labels of Aggregate Records**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One key difference between the two datasets is how they record players who played for multiple teams in a single season. Basketball-Reference provides both aggregated data and individual statistics for each team, while the NBA API dataset only includes an aggregated record.  \n",
    "\n",
    "Additionally, while both datasets provide aggregated records, Basketball-Reference labels them using a digit followed by \"TM\" to indicate the number of teams a player played for in that season. In contrast, the NBA API records only the last team the player played for that season.  \n",
    "\n",
    "To maintain consistency, we will align the Basketball-Reference dataset with the NBA API dataset.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_multi_team_data(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Handles players who played for multiple teams by assigning the team label\n",
    "    based on the last occurrence in the records for a single season.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Input DataFrame with columns including 'player', 'season',\n",
    "                           'team', and 'games_played'.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Cleaned DataFrame with multi-team players assigned the label\n",
    "                      of the team appearing last in the records for the season.\n",
    "    \"\"\"\n",
    "    # Separate multi-team players\n",
    "    aggregate_records_mask = df['team'].str.contains(r'^\\dTM$', na=False)\n",
    "    aggregate_records = df[aggregate_records_mask].copy()\n",
    "\n",
    "    # Identify the team for multi-team players based on the last occurrence\n",
    "    last_team_mapping = (\n",
    "        df[~aggregate_records_mask]\n",
    "        .groupby(['player', 'season'], group_keys=False, as_index=False)\n",
    "        .tail(1)[['player', 'season', 'team']]  # Use tail(1) to get the last row per group\n",
    "        .rename(columns={'team': 'last_team'})\n",
    "    )\n",
    "\n",
    "    # Create mapping\n",
    "    mapping = last_team_mapping.set_index(['player', 'season'])['last_team'].to_dict()\n",
    "\n",
    "    # Map and replace team labels\n",
    "    aggregate_records['team'] = aggregate_records.set_index(['player', 'season']).index.map(mapping)\n",
    "\n",
    "    # Identify multi-team players with multiple teams in a single season\n",
    "    multi_team = (\n",
    "        df[~aggregate_records_mask]\n",
    "        .groupby(['player', 'season'])['team']\n",
    "        .nunique()\n",
    "        .reset_index()\n",
    "        .query('team > 1')[['player', 'season']]\n",
    "    )\n",
    "\n",
    "    # Filter out rows for multi-team players directly using an index-based approach\n",
    "    df = df[~aggregate_records_mask]\n",
    "    df = df[~df.set_index(['player', 'season']).index.isin(multi_team.set_index(['player', 'season']).index)\n",
    "    ]\n",
    "\n",
    "    # Combine cleaned data\n",
    "    df = pd.concat([df, aggregate_records], ignore_index=True).sort_values(by=['player', 'season'])\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# Clean the data\n",
    "bball_ref_cleaned = clean_multi_team_data(bball_imputed_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Data Alignment: NBA Team Labels**\n",
    "Basketball-Reference uses different team labels than the NBA API. To merge the two datasets, these labels need to be standardised. Since the NBA API’s team labels are more intuitive, I will match the Basketball-Reference team labels to those used in the NBA API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find unique team labels\n",
    "print(\"Basketball-Reference Team Labels:\")\n",
    "print(f\"{np.sort(bball_ref_cleaned['team'].unique())}\\n\")\n",
    "print(\"NBA API Team Labels:\")\n",
    "print(np.sort(nba_dropped_df['team'].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The teams with differing labels include: Brooklyn, Charlotte, and Phoenix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping for known differences\n",
    "team_mapping = {'BRK': 'BKN', 'CHO': 'CHA', 'PHO': 'PHX'}\n",
    "\n",
    "# Align Basketball-Reference labels to NBA API labels\n",
    "bball_ref_cleaned['team'] = bball_ref_cleaned['team'].map(team_mapping).fillna(bball_ref_cleaned['team'])\n",
    "\n",
    "# Find differences in team labels\n",
    "print(f\"Shape of basketball-reference df: {bball_ref_cleaned.shape}\")\n",
    "print(f\"Shape of NBA API df: {nba_dropped_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Data Alignment: Player Name Suffix**\n",
    "The two data sources use different naming conventions for player suffixes. To ensure consistency, if a player's name includes a suffix in one dataset but not in the other, the suffix will be added to both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_suffix(no_suffix_df: pd.DataFrame, suffix_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Finds player suffixes from one dataframe and adds them to matching names in the other.\n",
    "    \"\"\"\n",
    "    suffix_dict = {}\n",
    "    for name in suffix_df['player'].unique():\n",
    "        if len(name.split(' ')) == 3:\n",
    "            first_name, last_name, suffix = name.split(' ')\n",
    "            suffix_dict[f\"{first_name} {last_name}\"] = suffix\n",
    "    \n",
    "    # Add suffix to matching players in no_suffix_df\n",
    "    no_suffix_df['player'] = no_suffix_df['player'].apply(\n",
    "        lambda name: f\"{name} {suffix_dict[name]}\" if name in suffix_dict else name\n",
    "    )\n",
    "\n",
    "    return no_suffix_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bball_ref_cleaned = add_suffix(bball_ref_cleaned, nba_dropped_df)\n",
    "nba_cleaned = add_suffix(nba_dropped_df, bball_ref_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3. Image Processing**  \n",
    "\n",
    "To enhance player visualisation, a CSV file is generated containing player names, nationalities, and image URLs for their official NBA headshots and national flags. The process follows these steps:  \n",
    "\n",
    "1. Extract player names, nationalities, and the latest NBA headshot URLs from the NBA API data  \n",
    "2. Retrieve country flag image URLs from [Country Flag URLs](https://www.kaggle.com/datasets/zhongtr0n/country-flag-urls)  \n",
    "3. Merge the two datasets on nationality to associate each player with their country’s flag image URL  \n",
    "4. Transform NBA headshots into circular images using the Python Imaging Library (PIL), upload the processed images to a [GitHub repository](https://github.com/jung-hyung-lee/nba-global-trends/tree/main/visualisation/circular_headshots), and update the CSV with the new URLs of the circular images  \n",
    "5. Save the final merged dataset as a CSV file for downstream Power BI visualisation  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Image Processing Pipeline**  \n",
    "\n",
    "The code below generates **NBA player headshot URLs**, merges **nationality data with flag images**, and processes **player images into circular PNGs**. It takes `nba_cleaned` as input, assigns **headshot and flag URLs**, downloads images, applies **circular cropping**, and saves the final dataset as `player_headshots.csv`, with processed images stored in `visualisation/circular_headshots/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_nba_images(\n",
    "    nba_df: pd.DataFrame, \n",
    "    flag_csv_path: str, \n",
    "    output_csv_path: str, \n",
    "    image_output_dir: str\n",
    "):\n",
    "    \"\"\"\n",
    "    Processes NBA player images by generating headshot URLs, merging with flag data, \n",
    "    and converting images into circular format.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Ensure the output directory exists\n",
    "    os.makedirs(image_output_dir, exist_ok=True)\n",
    "\n",
    "    # Generate player headshot URLs\n",
    "    image_df = nba_df[['player_id', 'player', 'nationality']].drop_duplicates()\n",
    "    image_df['player_id'] = image_df['player_id'].astype(int)\n",
    "    image_df['player_url'] = image_df['player_id'].apply(\n",
    "        lambda id: f\"https://ak-static.cms.nba.com/wp-content/uploads/headshots/nba/latest/260x190/{id}.png\"\n",
    "    )\n",
    "\n",
    "    # Load flag data and merge with player information\n",
    "    country_flags = pd.read_csv(flag_csv_path)\n",
    "    country_flags.rename(columns={\"URL\": \"flag_url\"}, inplace=True)\n",
    "    merge_on_country = pd.merge(image_df, country_flags, left_on=\"nationality\", right_on=\"Country\", how=\"left\")\n",
    "    merge_on_code = pd.merge(image_df, country_flags, left_on=\"nationality\", right_on=\"Code\", how=\"left\")\n",
    "    merge_on_country[\"flag_url\"] = merge_on_country[\"flag_url\"].fillna(merge_on_code[\"flag_url\"])\n",
    "\n",
    "    # Keep relevant columns\n",
    "    player_headshot_flag_df = merge_on_country[[\"player\", \"player_url\", \"flag_url\"]].drop_duplicates()\n",
    "\n",
    "    # Function to convert images into circular format\n",
    "    def make_circular_image(image_url: str, save_path: str):\n",
    "        \"\"\"\n",
    "        Downloads an image from a URL, crops it to a square, applies a circular mask, and saves it as a transparent PNG.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            response = requests.get(image_url)\n",
    "            if response.status_code != 200:\n",
    "                print(f\"Failed to download: {image_url}\")\n",
    "                return None\n",
    "\n",
    "            img = Image.open(BytesIO(response.content)).convert(\"RGBA\")\n",
    "\n",
    "            # Crop to a square\n",
    "            width, height = img.size\n",
    "            size = min(width, height)\n",
    "            img = img.crop(((width - size) // 2, (height - size) // 2, (width + size) // 2, (height + size) // 2))\n",
    "\n",
    "            # Apply circular mask\n",
    "            mask = Image.new(\"L\", (size, size), 0)\n",
    "            draw = ImageDraw.Draw(mask)\n",
    "            draw.ellipse((0, 0, size, size), fill=255)\n",
    "\n",
    "            circular_img = Image.new(\"RGBA\", (size, size), (0, 0, 0, 0))\n",
    "            circular_img.paste(img, (0, 0), mask)\n",
    "\n",
    "            # Save processed image\n",
    "            circular_img.save(save_path, format=\"PNG\")\n",
    "            return save_path\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing image {image_url}: {e}\")\n",
    "            return None\n",
    "\n",
    "    # Process player images\n",
    "    new_image_urls = []\n",
    "    for index, row in player_headshot_flag_df.iterrows():\n",
    "        player_name = row['player'].replace(\" \", \"_\").lower()\n",
    "        image_url = row['player_url']\n",
    "        save_path = os.path.join(image_output_dir, f\"{player_name}.png\").replace(\"\\\\\", \"/\")\n",
    "        processed_path = make_circular_image(image_url, save_path)\n",
    "        new_image_urls.append(processed_path if processed_path else image_url)\n",
    "\n",
    "    # Add processed image paths to DataFrame and save\n",
    "    player_headshot_flag_df[\"circular_player_url\"] = new_image_urls\n",
    "    player_headshot_flag_df.to_csv(output_csv_path, index=False)\n",
    "\n",
    "    print(f\"Processed images saved in: {image_output_dir}\")\n",
    "    print(f\"Updated CSV saved as: {output_csv_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Updating Image URLs**\n",
    "\n",
    "After uploading the circular images, the headshot image URLs are replaced with the new circular versions, and the updated CSV file is saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define CSV filepath\n",
    "csv_filepath = \"./data/player_headshots.csv\"\n",
    "csv_savepath = \"./data/updated_player_headshots.csv\"\n",
    "\n",
    "# Check if player_headshots.csv file exists and updated_player_headshots.csv file does not exist\n",
    "if os.path.exists(csv_filepath) and not os.path.exists(csv_savepath):\n",
    "    # Open player headshots CSV file\n",
    "    player_url_df = pd.read_csv(csv_filepath)\n",
    "\n",
    "    # Remove period in front of URLs inside circular_player_url column\n",
    "    player_url_df[\"circular_player_url\"] = player_url_df[\"circular_player_url\"].str.replace(r\"\\.\", \"\", n=1, regex=True)\n",
    "\n",
    "    # Replace player URL column with updated circular image URLs\n",
    "    player_url_df[\"player_url\"] = player_url_df[\"circular_player_url\"].apply(\n",
    "        lambda url: f\"https://raw.githubusercontent.com/jung-hyung-lee/nba-global-trends/refs/heads/main/{url}\"\n",
    "    )\n",
    "\n",
    "    # Drop circular_player_url\n",
    "    player_url_df.drop(columns=[\"circular_player_url\"], inplace=True)\n",
    "\n",
    "    # Save updated file\n",
    "    player_url_df.to_csv(csv_savepath, index=False)\n",
    "    print(f\"Updated circular image URLs saved to: {csv_savepath}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4. Exploratory Data Analysis (EDA)**  \n",
    "\n",
    "After merging the Basketball-Reference and NBA API DataFrames, this section explores underlying patterns and correlations within the data. It also examines the distribution of advanced statistics and provides key descriptive statistics to give a broad overview of league averages over the past three seasons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Merging**  \n",
    "\n",
    "The two DataFrames from Basketball-Reference and the NBA API will be merged based on `player name`, `season`, `team`, and `games played`. The resulting DataFrame will include all these details along with the **eight collected advanced statistics**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_diff_df = pd.merge(\n",
    "    bball_ref_cleaned,\n",
    "    nba_cleaned,\n",
    "    on=['player', 'season', 'team', 'games_played'],\n",
    "    how='outer'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown below, any remaining unmatched records—based on player name, season, team, and games played—stem from inherent inconsistencies between Basketball-Reference and the NBA API in how they record data. These discrepancies include differing team labels and slight variations in the number of games played. Since the number of such records is insignificant, they will be removed using an inner join."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_data_df = merged_diff_df[merged_diff_df.isnull().any(axis=1)]\n",
    "print(missing_data_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge DataFrames from Basketball-Reference and NBA API\n",
    "clean_df = pd.merge(\n",
    "    bball_ref_cleaned,\n",
    "    nba_cleaned,\n",
    "    on=['player', 'season', 'team', 'games_played'],\n",
    "    how='inner'\n",
    ")\n",
    "clean_df['games_played'] = clean_df['games_played'].astype(int)\n",
    "clean_df = clean_df.sort_values(by=['player', 'season'])\n",
    "\n",
    "# Save final DataFrame as csv\n",
    "if not os.path.exists(\"./data/clean_df.csv\"):\n",
    "    clean_df.to_csv(\"./data/clean_df.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **League-Wide Averages**  \n",
    "\n",
    "- **`Games Played`**: The average player participated in approximately 51 of 82 games  \n",
    "- **`PER`**: The mean of 13.6 is slightly below the theoretical league-wide average of 15, likely due to low-minute players dragging the unweighted average down  \n",
    "- **`TS%`**: The average of 56% aligns well with modern NBA scoring efficiency trends  \n",
    "- **`USG`**: Represents the percentage of team possessions a player uses. The 18.59% average is slightly below the league average of 20%, consistent with the dataset including bench players  \n",
    "- **`WS_48`**: The 0.086 average is close to the historical league average of 0.1 for a typical NBA player  \n",
    "- **`BPM`**: The unweighted mean of -1.12 indicates that the dataset contains many low-minute players  \n",
    "- **`VORP`**: Suggests most players contribute slightly above replacement-level production  \n",
    "- **`Offensive Rating`**: Represents an average of 109.66 points scored per 100 possessions while on the court  \n",
    "- **`Defensive Rating`**: Represents an average of 111.02 points allowed per 100 possessions while on the court  \n",
    "\n",
    "### **Variance**  \n",
    "\n",
    "Outside of true shooting percentage, all advanced statistics have high standard deviations, indicating that the data is more spread out from the mean rather than clustering tightly around it. In basketball analytics, high variance suggests significant differences in player performance, with large gaps in production between the best and worst players. If standard deviations were low, it would indicate that most players perform similarly, with few outliers—an inaccurate representation of the league.\n",
    "\n",
    "### **Outliers**  \n",
    "\n",
    "Extreme values suggest a mix of limited-minute players and exceptional performances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_variables = clean_df.select_dtypes([\"object\"]).columns.tolist()\n",
    "# For purposes of EDA, player name and team label columns are removed\n",
    "categorical_variables.remove('player')\n",
    "categorical_variables.remove('team')\n",
    "numeric_variables = clean_df.select_dtypes(['float64', 'int64']).columns.tolist()\n",
    "print(f\"Categorical Variables: {categorical_variables}\")\n",
    "print(f\"Numeric Variables: {numeric_variables}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Univariate Analysis: Histograms, Bar Charts, and Boxplots**  \n",
    "\n",
    "To better understand the distribution of each column and identify any outliers that may be present. \n",
    "\n",
    "### **Analysis of Histograms of Numeric Variables**  \n",
    "\n",
    "- **`Games Played`**: Left-skewed with the majority of players playing more than 55 games but a few playing very few games, pulling the mean down  \n",
    "- **`PER`**: Approximately normal with most PER scores centered around 13 or 14, very slightly positively skewed  \n",
    "- **`TS%`**: Approximately normal with most players shooting between 55 and 60%  \n",
    "- **`USG%`**: Right-skewed with a longer head than tail, most players averaging between 15-18% but a few superstars with extremely high usage pulling the mean up  \n",
    "- **`WS-48`**: Approximately normal with slight positive skewness  \n",
    "- **`BPM`**: Approximately normal with slight negative skewness  \n",
    "- **`VORP`**: Right-skewed with the majority of players having 0 VORP but a few with extremely high ratings pulling the mean up  \n",
    "- **`MIN`**: Approximately normal with a flat peak, indicating a large spread, which suggests the dataset includes all players from bench players to superstars  \n",
    "- **`Offensive Rating`**: Approximately normal with slight negative skewness  \n",
    "- **`Defensive Rating`**: Approximately normal with slight negative skewness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram of Numeric Variables\n",
    "num_bins = int(np.sqrt(len(clean_df)))\n",
    "\n",
    "# Define grid size\n",
    "rows = 4\n",
    "cols = 3\n",
    "\n",
    "# Create grid of subplots\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(18, 12))\n",
    "\n",
    "# Flatten 2D axes array for easier iteration\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, col in enumerate(numeric_variables):\n",
    "    sns.histplot(clean_df[col], bins=num_bins, ax=axes[i], edgecolor='black')\n",
    "    axes[i].set_title(f'Histogram of {col}', fontsize=12, fontweight='bold')\n",
    "    axes[i].set_xlabel(col.capitalize(), fontsize=10)\n",
    "    axes[i].set_ylabel('count', fontsize=10)\n",
    "\n",
    "# Remove empty subplots if any\n",
    "for i in range(len(numeric_variables), len(axes)):\n",
    "    fig.delaxes(axes[i])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Analysis of Boxplots of Numeric Variables**  \n",
    "\n",
    "- **`Games Played`**: A thicker box indicates a large spread around the median, with 50% of players playing approximately 38 to 68 games in a season  \n",
    "- **`PER`**: A thin box shows that PER scores cluster closely around the median, slightly below 15. A significant number of positive and negative outliers indicate that the dataset includes players of all calibres  \n",
    "- **`TS%`**: A thin box suggests that true shooting percentages are tightly distributed around the median of roughly 0.57. There are significantly more outliers with extremely low values, suggesting the unweighted mean may be skewed downward  \n",
    "- **`USG%`**: A relatively thick box shows that 50% of players use between 15% and 21% of team possessions, with a median of approximately 17%. A high number of outliers with extremely high usage reflects the presence of many superstar players in the NBA  \n",
    "- **`WS-48`**: A noticeably thin box with a median of approximately 0.1 suggests that most players perform close to the median. However, a greater number of negative outliers likely pulls the mean downward  \n",
    "- **`BPM`**: A thin box with a median slightly below 0. While there are many positive and negative outliers, the negative ones are both more frequent and more extreme  \n",
    "- **`VORP`**: A relatively thick box with a median of approximately 0.1. The median is closer to the 1st quartile than the 3rd quartile, indicating larger gaps in VORP between average players and key contributors (e.g., starters) compared to average players and minor contributors (e.g., bench players). This is reinforced by the large number of positive outliers (e.g., star players)  \n",
    "- **`MIN`**: A noticeably thick box with a median of 20 minutes per game, suggesting high variance in playing time. As expected, no outliers are present  \n",
    "- **`Offensive Rating` & `Defensive Rating`**: Both have extremely thin boxes with medians of approximately 110 points per 100 possessions. A significant number of negative outliers in both metrics suggest that the unweighted mean may be skewed downward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define figure size\n",
    "rows = 4\n",
    "cols = 3\n",
    "\n",
    "# Create grid of subplots\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(20, 16))\n",
    "\n",
    "# Flatten 2D axes array for easier iteration\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, col in enumerate(numeric_variables):\n",
    "    sns.boxplot(y=clean_df[col], ax=axes[i], color='skyblue')\n",
    "    axes[i].set_title(f\"Boxplot of {col}\", fontsize=12, fontweight='bold')\n",
    "    axes[i].set_ylabel(col.capitalize(), fontsize=10)\n",
    "\n",
    "for i in range(len(numeric_variables), len(axes)):\n",
    "    fig.delaxes(axes[i])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Analysis of Bar Charts for Categorical Variables**  \n",
    "\n",
    "Since the dataset includes a large number of nationalities, only the **top 10 most common** are visualised. The distribution highlights a significant dominance of **American players**, who make up approximately **76.7%** of the dataset. In contrast, the second most common nationality, **Canadian players**, account for only **4.2%**, revealing a stark disparity between the two groups.  \n",
    "\n",
    "One possible explanation for this gap is that many **foreign players may have played only a single season** before moving to other leagues. As a result, their records would have been excluded from the dataset, contributing to the observed imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain 10 most common nationalities\n",
    "top_n = 10\n",
    "top_nationalities = clean_df['nationality'].value_counts().nlargest(top_n)\n",
    "\n",
    "# Calculate percentages\n",
    "total_count = clean_df['nationality'].count()\n",
    "top_nationalities_percentage = (top_nationalities / total_count) * 100\n",
    "\n",
    "# Define figure size\n",
    "plt.figure(figsize=(10, 6))\n",
    "ax = sns.barplot(\n",
    "    x=top_nationalities.index,\n",
    "    y=top_nationalities.values,\n",
    "    palette='pastel',\n",
    "    edgecolor='black',\n",
    "    hue=top_nationalities.index,\n",
    "    legend=False\n",
    ")\n",
    "\n",
    "# Annotate bars with percentage values\n",
    "for i, value in enumerate(top_nationalities.values):\n",
    "    percentage = top_nationalities_percentage.iloc[i]\n",
    "    ax.text(i, value + 10, f\"{percentage:.1f}%\", ha='center', fontsize=10)\n",
    "\n",
    "plt.title(\"Top 10 Most Common Nationalities in Dataset\", fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Nationality', fontsize=12)\n",
    "plt.ylabel('Count', fontsize=12)\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution of data across seasons is relatively even, with the 2022-23 season having the highest number of records at approximately 510 and the 2021-22 season the lowest at around 440.  \n",
    "\n",
    "This fluctuation in player count across seasons is expected, as NBA teams do not have a fixed roster size. Each team can have up to 15 standard contract players, plus two additional two-way players, bringing the maximum roster size to 17.  \n",
    "\n",
    "Since the dataset includes players who participated in at least two seasons, one possible reason for the higher count in 2022-23 is the overlap of players appearing in both 2021-22 and 2022-23, as well as those in 2022-23 and 2023-24, but not necessarily all three seasons. This overlap may have resulted in a higher number of unique player records in the middle season (2022-23) compared to the other two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure seasons are sorted correctly\n",
    "season_order = [\"2021-22\", \"2022-23\", \"2023-24\"]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(data=clean_df, x='season', order=season_order, hue='season', legend=False, palette='pastel', edgecolor='black')\n",
    "\n",
    "plt.title(\"Count Plot of Past 3 NBA Seasons\", fontsize=14, fontweight='bold')\n",
    "plt.xlabel(\"Season\", fontsize=12)\n",
    "plt.ylabel(\"Count\", fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Multivariate Analysis: Heatmap of Correlation Coefficients**  \n",
    "\n",
    "A heatmap is created to visualise relationships between numerical variables in the dataset. This can highlight the strength and direction of relationships, helping to identify multicollinearity and underlying patterns, providing valuable insights for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define DataFrame with only numeric columns\n",
    "numeric_df = clean_df.select_dtypes(['number'])\n",
    "\n",
    "# Compute correlation matrix\n",
    "corr_mat = numeric_df.corr()\n",
    "\n",
    "# Define figure size\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Create heatmap\n",
    "sns.heatmap(\n",
    "    corr_mat,\n",
    "    annot=True,\n",
    "    fmt=\".2f\",\n",
    "    cmap=\"coolwarm\",\n",
    "    linewidths=0.5,\n",
    "    cbar=True\n",
    ")\n",
    "\n",
    "plt.title(\"Heatmap of Correlation Coefficients\", fontsize=14, fontweight=\"bold\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Analysis of Heatmap**  \n",
    "\n",
    "#### **Key Observations**  \n",
    "\n",
    "- **Strong Positive Correlations:**  \n",
    "  A strong correlation exists between **Box Plus-Minus (BPM), Win Shares per 48 Minutes (WS/48), and Player Efficiency Rating (PER)**, indicating that higher efficiency ratings align with greater contributions to team success.  \n",
    "  - **BPM & PER**: **0.89**  \n",
    "  - **WS/48 & BPM**: **0.87**  \n",
    "  - **WS/48 & PER**: **0.86**  \n",
    "  These relationships are expected, as **BPM and WS/48 reflect overall impact**, while **PER measures individual per-minute efficiency**, making them naturally aligned in high-performing players.  \n",
    "\n",
    "- **Moderate Correlations:**  \n",
    "  - **Minutes Played per Game (MIN) & Total Games Played**: **0.63**  \n",
    "  - **MIN & Value Over Replacement Player (VORP)**: **0.59**  \n",
    "  Players who log more minutes per game tend to appear in more games overall and accumulate higher **VORP**, a cumulative impact metric. The correlation between **VORP and minutes played** is expected, as greater playing time naturally results in higher total contributions.  \n",
    "\n",
    "- **Weak or No Correlation:**  \n",
    "  - **Defensive Rating (def_rating)** shows little correlation with most advanced metrics.  \n",
    "  - A slight negative correlation with **BPM (-0.05)** suggests that **defensive impact may not be well captured in conventional impact metrics**, potentially indicating a bias toward offensive contributions.  \n",
    "\n",
    "### **Key Takeaways & Next Steps**  \n",
    "\n",
    "✔️ **Most advanced statistics are positively correlated, except for Defensive Rating.**  \n",
    "✔️ **Offensive performance appears to be weighted more heavily than defense in standard advanced stats.**  \n",
    "✔️ **To ensure a more balanced player evaluation**, **composite score weights** must be adjusted to enhance the influence of defensive metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5. Storing, Filtering, and Retrieving Data**\n",
    "\n",
    "**Connect** to a PostgreSQL database, **create tables** to store cleaned data, and **execute queries** with filters to retrieve and store data in DataFrames."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **PostgreSQL-Related Functions**\n",
    "\n",
    "Define `environment variables` and `functions` necessary to store, filter, and retrieve data from a PostgreSQL database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read environment variables\n",
    "DB_NAME: Optional[str] = os.getenv(\"DB_NAME\")\n",
    "DB_USER: Optional[str] = os.getenv(\"DB_USER\")\n",
    "DB_PASSWORD: Optional[str] = os.getenv(\"DB_PASSWORD\")\n",
    "DB_HOST: Optional[str] = os.getenv(\"DB_HOST\")\n",
    "DB_PORT: Optional[str] = os.getenv(\"DB_PORT\")\n",
    "\n",
    "def connect_db():\n",
    "    \"\"\"Establishes a connection to the PostgreSQL database\"\"\"\n",
    "    try:\n",
    "        conn = psycopg2.connect(\n",
    "            dbname=DB_NAME,\n",
    "            user=DB_USER,\n",
    "            password=DB_PASSWORD,\n",
    "            host=DB_HOST,\n",
    "            port=DB_PORT\n",
    "        )\n",
    "        print(\"Connection to the database was successful.\")\n",
    "        return conn\n",
    "    except OperationalError as e:\n",
    "        print(f\"Error: Failed to connect to the PostgreSQL database.\\nDetails: {e}\")\n",
    "        return None\n",
    "\n",
    "def table_exists(cursor: psycopg2.extensions.cursor, table_name: str) -> bool:\n",
    "    \"\"\"Checks if a table exists in the PostgreSQL database\"\"\"\n",
    "    query = \"\"\"\n",
    "    SELECT EXISTS(\n",
    "        SELECT FROM information_schema.tables\n",
    "        WHERE table_name = %s\n",
    "    );\n",
    "    \"\"\"\n",
    "    cursor.execute(query, (table_name,))\n",
    "    return cursor.fetchone()[0]\n",
    "\n",
    "def check_and_create_table(cursor: psycopg2.extensions.cursor, conn: psycopg2.extensions.connection) -> None:\n",
    "    \"\"\"Checks if the required tables exist and creates them if not\"\"\"\n",
    "    tables: dict[str, str] = {\n",
    "        \"all_three_seasons\": \"\"\"\n",
    "            CREATE TABLE all_three_seasons (\n",
    "                player VARCHAR(50) NOT NULL,\n",
    "                team VARCHAR(10) NOT NULL,\n",
    "                games_played INT NOT NULL,\n",
    "                per FLOAT NOT NULL,\n",
    "                ts FLOAT NOT NULL,\n",
    "                usg FLOAT NOT NULL,\n",
    "                ws_48 FLOAT NOT NULL,\n",
    "                bpm FLOAT NOT NULL,\n",
    "                vorp FLOAT NOT NULL,\n",
    "                season VARCHAR(10) NOT NULL,\n",
    "                min FLOAT NOT NULL,\n",
    "                off_rating FLOAT NOT NULL,\n",
    "                def_rating FLOAT NOT NULL\n",
    "            );\n",
    "        \"\"\"\n",
    "    }\n",
    "\n",
    "    for table, create_query in tables.items():\n",
    "        if not table_exists(cursor, table):\n",
    "            cursor.execute(create_query)\n",
    "            conn.commit()\n",
    "            print(f\"Table `{table}` created.\")\n",
    "\n",
    "def load_data(engine: create_engine) -> None:\n",
    "    \"\"\"Loads data from CSV files into the PostgreSQL database\"\"\"\n",
    "    datasets: dict[str, str] = {\n",
    "        \"all_three_seasons\": \"./data/clean_df.csv\"\n",
    "    }\n",
    "    try:\n",
    "        for table, csv_path in datasets.items():\n",
    "            df: pd.DataFrame = pd.read_csv(csv_path)\n",
    "            df.to_sql(table, engine, if_exists=\"replace\", index=False)\n",
    "            print(f\"Data loaded into `{table}`\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load data into `{table}`: {e}\")\n",
    "\n",
    "def retrieve_data(engine: create_engine, query: str, var_name: str = \"DataFrame\") -> Optional[pd.DataFrame]:\n",
    "    \"\"\"Executes the given SQL query and returns the result as a DataFrame\"\"\"\n",
    "    try:\n",
    "        with engine.connect() as conn:\n",
    "            df: pd.DataFrame = pd.read_sql(query, conn)\n",
    "            print(f\"Successfully retrieved data and stored in '{var_name}'\")\n",
    "            return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error executing query: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Main Execution**  \n",
    "\n",
    "This process utilises PostgreSQL functions to:  \n",
    "1. **Connect to the `nba_players` database**  \n",
    "2. **Create necessary tables** if they do not already exist  \n",
    "3. **Load CSV data** into these tables  \n",
    "4. **Execute SQL queries** to retrieve relevant data  \n",
    "5. **Store query results as DataFrames** for further analysis  \n",
    "\n",
    "The `weighted_query` below calculates the **weighted averages** of advanced statistics based on total minutes played, applying filters to ensure only significant contributors are considered. Players must meet the following criteria:  \n",
    "- **At least 15 minutes per game** (out of 48 minutes per game)  \n",
    "- **At least 41 games played** (half of an 82-game season)  \n",
    "- **Participation in at least 2 of the last 3 seasons**\n",
    "\n",
    "The `main_query` retrieves **all player-season records** meeting the **same criteria** while also:\n",
    "  - Counting **eligible seasons played** (`seasons_played`)  \n",
    "  - Including **all season-wise statistics**  \n",
    "\n",
    "After execution, the script saves results in DataFrames (`weighted_average_df` & `final_df`) for further analysis.\n",
    "\n",
    "These weighted averages will be used to compute a **composite score**, providing a more comprehensive measure of player performance over the past three seasons to support player rankings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main execution\n",
    "conn = connect_db()\n",
    "\n",
    "# Define query to obtain weighted averages of all advanced statistics based on total minutes played per season\n",
    "weighted_query = \"\"\"\n",
    "    WITH total_minutes_cte AS(\n",
    "        SELECT\n",
    "            *,\n",
    "            (min * games_played) AS total_minutes\n",
    "        FROM all_three_seasons\n",
    "        WHERE                  -- Filter for players that played at least 15 minutes per game and 41 out of 82 games per season\n",
    "            min >= 15 AND\n",
    "            games_played >= 41\n",
    "    )\n",
    "    SELECT\n",
    "        player,\n",
    "        nationality,\n",
    "        CASE\n",
    "            WHEN nationality = 'USA' THEN 'USA'\n",
    "            ELSE 'International'\n",
    "        END AS player_origin,\n",
    "        SUM(total_minutes * per) / SUM(total_minutes) AS avg_per,       \n",
    "        SUM(total_minutes * ts) / SUM(total_minutes) AS avg_ts,       \n",
    "        SUM(total_minutes * usg) / SUM(total_minutes) AS avg_usg,\n",
    "        SUM(total_minutes * ws_48) / SUM(total_minutes) AS avg_ws_48,\n",
    "        SUM(total_minutes * bpm) / SUM(total_minutes) AS avg_bpm,\n",
    "        SUM(total_minutes * vorp) / SUM(total_minutes) AS avg_vorp,\n",
    "        SUM(total_minutes * off_rating) / SUM(total_minutes) AS avg_off_rating,\n",
    "        SUM(total_minutes * def_rating) / SUM(total_minutes) AS avg_def_rating\n",
    "    FROM total_minutes_cte\n",
    "    WHERE total_minutes > 0\n",
    "    GROUP BY player, nationality, player_origin\n",
    "    HAVING COUNT(DISTINCT season) >= 2\n",
    "    ORDER BY player\n",
    "    ;\n",
    "\"\"\"\n",
    "\n",
    "main_query = \"\"\"\n",
    "    WITH player_season_counts AS (\n",
    "        SELECT \n",
    "            player, \n",
    "            COUNT(DISTINCT season) AS seasons_played\n",
    "        FROM all_three_seasons\n",
    "        WHERE min >= 15 \n",
    "        AND games_played >= 41\n",
    "        GROUP BY player\n",
    "    )\n",
    "    SELECT \n",
    "        a.*\n",
    "        , CASE \n",
    "            WHEN a.nationality = 'USA' THEN 'USA' \n",
    "            ELSE 'International' \n",
    "        END AS player_origin\n",
    "        , psc.seasons_played\n",
    "    FROM all_three_seasons a\n",
    "    JOIN player_season_counts psc \n",
    "        ON a.player = psc.player\n",
    "    WHERE psc.seasons_played >= 2  \n",
    "    AND a.min >= 15 \n",
    "    AND a.games_played >= 41;\n",
    "\"\"\"\n",
    "\n",
    "if conn is None:\n",
    "    print(\"Connection failed. Exiting...\")\n",
    "    sys.exit\n",
    "\n",
    "try:\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # Check and create tables if they do not exist\n",
    "    check_and_create_table(cursor, conn)\n",
    "\n",
    "    # Close psycopg2 connection\n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "\n",
    "    # SQLAlchemy connection for efficient data operations\n",
    "    engine = create_engine(f\"postgresql://{DB_USER}:{DB_PASSWORD}@{DB_HOST}:{DB_PORT}/{DB_NAME}\")\n",
    "\n",
    "    # Load data into tables\n",
    "    load_data(engine)\n",
    "\n",
    "    # Execute query and store results in DataFrame\n",
    "    weighted_avg_df = retrieve_data(engine, weighted_query, var_name=\"weighted_average_df\")\n",
    "    final_df = retrieve_data(engine, main_query, var_name=\"final_df\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Unexpected error: {e}\")\n",
    "\n",
    "finally:\n",
    "    print(\"Script execution completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **6. Ranking Players**\n",
    "\n",
    "Compute the `composite score` for each player based on **normalised weighted-averages** of advanced statistics to **rank players** that played at least 2 out of the last 3 NBA seasons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Normalisation**\n",
    "\n",
    "Before calculating the composite score, the weighted-averages must be normalised in order to **prevent large statistics such as PER and BPM from dominating the score**. In other words, normalising the averages ensures fair contribution from all statistics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalise advanced statistics\n",
    "scaler = MinMaxScaler()\n",
    "metrics = ['per', 'ts', 'usg', 'ws_48', 'bpm', 'vorp', 'off_rating', 'def_rating']\n",
    "# Create copy to normalise advanced statistics\n",
    "normalised_final_df = final_df.copy()\n",
    "normalised_final_df[metrics] = scaler.fit_transform(final_df[metrics])\n",
    "\n",
    "# Normalise weighted-averages of all advanced statistics\n",
    "scaler = MinMaxScaler()\n",
    "avg_metrics = ['avg_per', 'avg_ts', 'avg_usg', 'avg_ws_48', 'avg_bpm', 'avg_vorp', 'avg_off_rating', 'avg_def_rating']\n",
    "# Create copy to normalise weighted advanced statistics\n",
    "normalised_avg_df = weighted_avg_df.copy()\n",
    "normalised_avg_df[avg_metrics] = scaler.fit_transform(weighted_avg_df[avg_metrics])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Composite Score**\n",
    "\n",
    "The **composite score** is constructed by first **normalising** the advanced statistic to ensure consistency across different metrics.  \n",
    "\n",
    "The **correlation heatmap** reveals that **defensive rating** has little to no correlation with the other seven statistics, which primarily reward offensive contributions. This highlights a key imbalance, as **modern NBA metrics tend to favour offence-focused players.**  \n",
    "\n",
    "To address this, the composite score is **weighted 70% towards offensive impact** (distributed across the seven offence-oriented metrics) and **30% towards defensive rating**. This **70:30 split** ensures that **defensive effectiveness receives meaningful consideration**, while still reflecting the league’s offensive emphasis. This approach accounts for **both ends of the court** without disproportionately favouring one over the other.\n",
    "\n",
    "| **Advanced Statistic**         | **Weight (%)** | **Rationale** |\n",
    "|---------------------------|------------|--------------------------------------------------------------------------|\n",
    "| Defensive Rating      | 30%        | Balances offensive bias by ensuring defensive impact is strongly considered |\n",
    "| PER                  | 8%         | Measures overall impact but highly correlated with BPM and WS/48, requiring reduced weight |\n",
    "| BPM                  | 8%         | Accounts for box score impact but overlaps significantly with PER and WS/48 |\n",
    "| VORP                 | 8%         | Captures overall value over replacement level but correlates with PER and BPM |\n",
    "| True Shooting % (TS%) | 11.5%      | Reflects scoring efficiency, an independent metric not tied to counting stats |\n",
    "| Usage % (USG%)       | 11.5%      | Shows offensive role and involvement, providing unique insight |\n",
    "| Win Shares per 48 (WS/48) | 11.5%  | Rewards consistent contributions but is highly tied to PER and BPM |\n",
    "| Offensive Rating      | 11.5%      | Measures offensive efficiency directly, adding a distinct dimension |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define weights for each metric\n",
    "weights = [0.08, 0.115, 0.115, 0.115, 0.08, 0.08, 0.115, 0.3]\n",
    "main_metric_weight = dict(zip(metrics, weights))\n",
    "weighted_metric_weight = dict(zip(avg_metrics, weights))\n",
    "\n",
    "# Calculate composite score for all advanced statistics\n",
    "final_df[\"composite_score\"] = normalised_final_df[\n",
    "    list(main_metric_weight.keys())\n",
    "].mul(main_metric_weight).sum(axis=1)\n",
    "final_df[\"composite_score\"] = final_df[\"composite_score\"].round(4)\n",
    "\n",
    "# Calculate composite score for all weighted statistics\n",
    "weighted_avg_df[\"composite_score\"] = normalised_avg_df[\n",
    "    list(weighted_metric_weight.keys())\n",
    "].mul(weighted_metric_weight).sum(axis=1)\n",
    "weighted_avg_df[\"composite_score\"] = weighted_avg_df[\"composite_score\"].round(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Saving the data as csv files**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv(\"./data/final_df.csv\", index=False)\n",
    "weighted_avg_df.to_csv(\"./data/weighted_df.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".nba_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
